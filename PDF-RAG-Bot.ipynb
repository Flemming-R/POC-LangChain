{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import langchain_ollama\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import FewShotPromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_milvus import Milvus\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                     \tID          \tSIZE  \tMODIFIED     \n",
      "deepseek-r1:8b           \t28f8fd6cdc67\t4.9 GB\t25 hours ago\t\n",
      "llama3.2:1b-instruct-q4_0\t53f2745c8077\t770 MB\t3 months ago\t\n",
      "llama3.2:1b              \tbaf6a787fdff\t1.3 GB\t3 months ago\t\n",
      "llama3.1:8b              \t42182419e950\t4.7 GB\t4 months ago\t\n",
      "mistral:instruct         \tf974a74358d6\t4.1 GB\t4 months ago\t\n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(\n",
    "    model=\"llama3.1:8b\",\n",
    "    temperature=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"./POC-LangChain/PDF_for_RAG-bot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 14 pages from 1 PDFs.\n"
     ]
    }
   ],
   "source": [
    "df_files = [f for f in os.listdir(folder_path) if f.endswith(\".pdf\")]\n",
    "\n",
    "pdf_files = [f for f in os.listdir(folder_path) if f.endswith(\".pdf\")]\n",
    "documents = []\n",
    "for pdf in pdf_files:\n",
    "    pdf_path = os.path.join(folder_path, pdf)\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "print(f\"Loaded {len(documents)} pages from {len(pdf_files)} PDFs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6c658230124a82a29ff5d9e5add1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6680cc6f01a47689239d67305b7d6ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\flemm\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-lernen-GE3QnVly-py3.11\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\flemm\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e48e283d3054c239d8d68ba9bf43bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9f39b0d73546b597100fac6bcfc2bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b660156efd124e08885b06843f153163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa601ad40144fa2be1864711e8f3a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8f9e66be1043e087583b933a67d71f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69689f554d3465dac70e1655b85950d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c78fccaf472345ceb7b60ea0d2cfd525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a8b1dd855248fc8fb1328c5abeaa73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baed429e7fa844b3a381850589b7980e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e031789bb14c2b80747f04a019e399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use a free embedding model from Hugging Face\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text splitter (adjust chunk_size as needed)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "# Split documents into smaller chunks\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Convert document chunks into embeddings\n",
    "vector_store = FAISS.from_documents(chunks, embedding_model)\n",
    "\n",
    "# Save the vector store\n",
    "vector_store.save_local(\"faiss_index\")\n",
    "\n",
    "print(\"Embeddings generated and saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS vector store loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load FAISS vector store with safe deserialization\n",
    "vector_store = FAISS.load_local(\"faiss_index\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Convert FAISS store into a retriever\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "print(\"FAISS vector store loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is NL2SQL360?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"Use the given context only to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is NL2SQL360?',\n",
       " 'context': [Document(id='917966a1-aaf9-4038-9fd9-e73c81609d4d', metadata={'source': './POC-LangChain/PDF_for_RAG-bot\\\\Li et al. - 2024 - The Dawn of Natural Language to SQL Are We Fully .pdf', 'page': 0, 'page_label': '1'}, page_content='databases. The emergence of Large Language Models has intro-\\nduced a novel paradigm in nl2sql tasks, enhancing capabilities\\ndramatically. However, this raises a critical question:Are we fully\\nprepared to deploy nl2sql models in production?\\nTo address the posed questions, we present a multi-anglenl2sql\\nevaluation framework, NL2SQL360, to facilitate the design and test\\nof new nl2sql methods for researchers. Through NL2SQL360, we\\nconduct a detailed comparison of leading nl2sql methods across a'),\n",
       "  Document(id='fe7cb7eb-d418-426f-b13d-bd4efcfc6c57', metadata={'source': './POC-LangChain/PDF_for_RAG-bot\\\\Li et al. - 2024 - The Dawn of Natural Language to SQL Are We Fully .pdf', 'page': 0, 'page_label': '1'}, page_content='range of application scenarios, such as different data domains and\\nsql characteristics, offering valuable insights for selecting the most\\nappropriate nl2sql methods for specific needs. Moreover, we ex-\\nplore the nl2sql design space, leveraging NL2SQL360 to automate\\nthe identification of an optimal nl2sql solution tailored to user-\\nspecific needs. Specifically,NL2SQL360 identifies an effectivenl2sql\\nmethod, SuperSQL, distinguished under the Spider dataset using the'),\n",
       "  Document(id='4848d75b-f69d-45ab-81f7-297669e001e6', metadata={'source': './POC-LangChain/PDF_for_RAG-bot\\\\Li et al. - 2024 - The Dawn of Natural Language to SQL Are We Fully .pdf', 'page': 2, 'page_label': '3'}, page_content='models across different benchmarks from multiple perspectives.\\nNL2SQL360 can help researchers and practitioners better evaluate\\nnl2sql models on their specific scenarios, uncover insightful exper-\\nimental findings, and design a superior nl2sql model that is more\\nrobust than SOTA solutions. Our contributions are as follows.\\n(1) NL2SQL360: multi-angle NL2SQL evaluation. We design a\\ntestbed, NL2SQL360, for fine-grained evaluation ofnl2sql solutions.'),\n",
       "  Document(id='e5670e69-473f-4240-b601-ddc13c3b1ab3', metadata={'source': './POC-LangChain/PDF_for_RAG-bot\\\\Li et al. - 2024 - The Dawn of Natural Language to SQL Are We Fully .pdf', 'page': 3, 'page_label': '4'}, page_content='3 NL2SQL360: A TESTBED FOR NL2SQL\\nFigure 4 shows the framework of our testbed, NL2SQL360, compris-\\ning six core components, as discussed below.\\nwww.presentationgo.com\\n6x2 Two-Level Chart –Slide Template\\nSpiderBIRD\\nNL2SQL360Lifecycle\\nModel ZooMetrics\\nFilterEvaluatorAnalysis\\nDAIL-SQLDINSQLC3SQLWikiSQLDr.SpiderGraphix-T5RESDSQLExecutionAccuracy\\nExact-MatchAccuracy\\nSQLComplexity CharacteristicsDatabaseDomains\\nQuantitativeEvaluation\\nVisualizationDashboard\\nLeaderboard\\nQualitativeEvaluation\\nDataset'),\n",
       "  Document(id='23d0595f-cde3-409f-b435-a64142173ee5', metadata={'source': './POC-LangChain/PDF_for_RAG-bot\\\\Li et al. - 2024 - The Dawn of Natural Language to SQL Are We Fully .pdf', 'page': 3, 'page_label': '4'}, page_content='pared on well-established benchmarks and customized datasets.\\n(3) Limited Exploration of the NL2SQL Design Space.Current\\nnl2sql research and practice have limited exploration of thenl2sql\\nframework’s design space using both LLM and PLM-based modules.\\nThis restricts our understanding of how different architectural and\\nfunctional modules from both LLM and PLM can be synergistically\\nincorporated to enhance nl2sql solutions.\\n3 NL2SQL360: A TESTBED FOR NL2SQL')],\n",
       " 'answer': \"NL2SQL360 is a multi-angle evaluation framework for Natural Language to Structured Query Language (nl2sql) tasks. It's designed to facilitate the design and testing of new nl2sql methods, allowing researchers to compare leading nl2sql methods across various application scenarios and identify the most suitable method for specific needs.\"}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": query})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-lernen-GE3QnVly-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
